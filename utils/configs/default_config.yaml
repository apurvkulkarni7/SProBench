##############################
# Benchmark Settings
##############################
#
# The following settings are used to configure the benchmarking process.
#
# - Create the logging directories in the same location instead of at "/tmp"
#   location
# - debug_mode: To create the temporary logging files at the specified location
#   instead of "/tmp"
# - logging_level: To set the logging level in the benchmark
#   1: info, 2: warn, 3: debug
# - benchmark_runtime_min: The minimum runtime for the benchmark in minutes.
#   This is the minimum amount of time that the benchmark will run for.
# - metric_logging_interval_sec: The interval at which metrics are logged in
#   seconds.
# - runs_per_configuration: The number of times each configuration is run.
#   This allows the benchmark to be repeated multiple times with the same
#   configuration.
# - total_load_hz: The total workload in Hz.
debug_mode: "true"
logging_level: 1
benchmark_runtime_min: 5
metric_logging_interval_sec: 10
runs_per_configuration: 1
total_workload_hz:
  - 2
tmp_dir: /tmp
##############################
# Generator Settings
##############################
#
# The following settings are used to configure the generators used in the
# benchmarking process.
#
# - type: The type of generator used (constant, burst, random).
# - load_hz: workload per generator. Total number of generators will be
#   calculated based on load_Hz and total_load_hz.
# - cpu: The number of CPUs allocated to each generator.
# - threads_per_cpu_num: The number of threads per CPU for each
#   generator.
# - mem: The memory required for each generator in GB.
# - only_data_generator: A flag to indicate if only data generator is used.
#   This can be used to test and optimize generator.
# - record_size_bytes: The size of each record in bytes. Minimum size for
#   sensor data is 27 bytes.
generator:
  type: constant
  load_hz: 2
  cpu: 1
  threads_per_cpu_num: 1
  memory_gb: 2
  record_size_bytes: 27
  only_data_generator: "false"
##############################
# Processor Settings
##############################
#
# Controls the benchmarking stream processing engine. Settings below become
# inactive when 'only_data_generator' is enabled.
# - processing_type: Processing Workload Types
#   - P0: Passthrough (no transformation)
#   - P1: CPU-intensive processing
#   - P2: Memory-intensive stateful operations
# -framework: Supported Frameworks
#   - messagebroker : Basic message routing
#   - flink         : Apache Flink engine
#   - spark         : Apache Spark Streaming
#   - kafkastream   : Kafka Streams API
# - master: Master node configuration
#   - cpu       : Number of CPUs for master processes
#   - memory_gb : Memory in GB for master processes
# - worker: Worker configuration
#   - instances    : Number of worker processes in cluster
#                    (one/physical_node currently)
#   - parallelism : Parallelism per worker
#   - memory_gb   : Memory in GB per worker process
# - stateful_dir: Directory for stateful operations (checkpoints, state storage).
#                 For P2 processing.
# - window_length_ms: Total duration of processing window. For P2 processing.
#                     For P2 processing.
# - window_advance_ms: Slide interval (tumbling window if equal to length). 
#                      For P2 processing.
# - num_cpus_spare: Reserve CPUs per worker node for auxiliary processes (OS, 
#                   monitoring, etc)
# - mem_node_spare: Reserve memory (GB) per worker node for system operations
stream_processor:
  processing_type: P2
  framework:
    #- flink
    # - kafkastream
    - sparkstrucstream
  master:
    cpu: 1
    memory_gb: 1
  worker:
    instances:
      - 1
    memory_gb: 1
    parallelism:
      - 1
  stateful_dir: /tmp
  window_length_ms: 5000
  window_advance_ms: 5000
num_cpus_spare: 1
mem_node_spare: 1
##############################
# Message broker Settings
##############################
#
# The following settings are used to configure the Kafka setup used in the
# benchmarking process.
#
# - kafka_source_topics: The source topics for Kafka from which data will
#   be read.
# - kafka_sink_topics: The sink topics for Kafka. This specifies the topics
#   to which data will be written.
# - num_partition: Number of partition for the given topic. The value 'processor'
#   will set the partition number equal to streaming framework parallelism
kafka:
  cpu: 1
  memory_gb: 1
  source_topics:
    - name: eventsIn
      num_partition: processor
  sink_topics:
    - name: eventsOut
      num_partition: processor
##############################
# Frameworks Setup
##############################
#
# The following options are available for each framework:
#
# - version: The version of the framework to be used.
# - local_setup: The setup options for local environments.
#   - path: The path to the framework installation. If set to 'default', the
#     installer will attempt to install the framework at the default location.
# - slurm_setup: The setup options for HPC environments.
#   - use_module_system: Whether to use the module system for loading the
#     framework. If set to false, then it will be installed on location
#     mentioned in path parameter
#   - module_name_version: The version of the module to be used.
#   - dependent_modules: A list of dependent modules required by the framework.
#   - path: The path to the framework installation. If set to 'default', the
#     installer will attempt to install the framework at the default location
#     <benchmark_root_dir>/frameworks/<framework_name>.
#   - custom_module_path: for slurm setup. This is an advance option in case
#     the user installs the frameworks on different location. This path will be
#     added to MODULEPATH.
frameworks:
  java:
    version: 11.0.2
    local_setup:
      path: default
    slurm_setup:
      use_module_system: true
      module_name_version: Java/11.0.20
      dependent_modules:
        - release/24.04
      path: default
  maven:
    version: 3.9.6
    local_setup:
      path: default
    slurm_setup:
      use_module_system: true
      module_name_version: Maven/3.9.6
      dependent_modules:
        - depency1
        - depency2
      path: default
  flink:
    version: 1.19.1
    local_setup:
      path: default
    slurm_setup:
      use_module_system: true
      module_name_version: Flink/1.19.1
      dependent_modules:
        - depency1
        - depency2
      path: default
  kafka:
    version: 3.6.1
    local_setup:
      path: default
    slurm_setup:
      use_module_system: true
      module_name_version: Kafka/3.6.1-scala-2.13
      dependent_modules:
        - depency1
        - depency2
      path: default
  spark:
    version: 3.5.3
    local_setup:
      path: default
    slurm_setup:
      use_module_system: true
      module_name_version:
      dependent_modules:
        - depency1
        - depency2
      path: default
  custom_module_path:
    - /path/to/custom/easybuild/installation
##############################
# Slurm Job Allocation Setup
##############################
#
# The following settings are used to configure the HPC job setup used in the
# benchmarking process.
#
# - project: The project name for the HPC job.
# - exclusive_jobs: A flag to indicate if jobs are exclusive. Should be used 
#   inside double quotes
# - chained_jobs: A flag to indicate if jobs are chained or run independently.
# - multithreading: A flag to indicate if multithreading is enabled.
slurm_setup:
  project: p_scads
  exclusive_jobs: "false"
  chained_jobs: "true"
  multithreading: "false"